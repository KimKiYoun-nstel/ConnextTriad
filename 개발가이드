결론: 4·5단계를 한 번에 적용한다. Listener 기본 유지. WaitSet은 “스텁 + 주석 + 훅”만 둔다. 5-2 모니터링/로깅은 즉시 활성화된다.

# A. ReceiverFactory 도입(4단계)

## A-1. 새 파일 추가

### `include/async/dds_receiver_interface.hpp`

```cpp
#pragma once
namespace rtpdds { namespace async {

enum class DdsReceiveMode { Listener, WaitSet };

struct IDdsReceiver {
    virtual ~IDdsReceiver() = default;
    virtual void activate() = 0;    // 수신 시작
    virtual void deactivate() = 0;  // 수신 중지
};

}} // namespace
```

### `include/async/receiver_factory.hpp`

```cpp
#pragma once
#include <memory>
#include "dds_receiver_interface.hpp"

namespace rtpdds { class DdsManager; }
namespace rtpdds { namespace async {

std::unique_ptr<IDdsReceiver>
create_receiver(DdsReceiveMode mode, rtpdds::DdsManager& mgr);

}} // namespace
```

### `src/async/receiver_factory.cpp`

```cpp
#include "async/receiver_factory.hpp"
#include "triad_log.hpp"   // LOG_INF/WRN

namespace rtpdds { class DdsManager; }

namespace rtpdds { namespace async {

namespace {

struct ListenerReceiver final : IDdsReceiver {
    void activate()   override { LOG_INF("DDSRX","ListenerReceiver activate"); }
    void deactivate() override { LOG_INF("DDSRX","ListenerReceiver deactivate"); }
};

// TODO(WaitSet): Modern C++ API의 ReadCondition/WaitSet 사용.
//  - 각 DataReader 핸들 접근 필요(DdsManager accessor).
//  - dds::sub::cond::ReadCondition 생성 후 dds::core::cond::WaitSet에 attach.
//  - 전용 스레드에서 wait()/dispatch() 루프.
//  - 루프 내 read()/take() → AsyncEventProcessor::post(...)
// 참고 심벌: dds::core::cond::WaitSet::dispatch, dds::sub::cond::ReadCondition.
struct WaitSetReceiver final : IDdsReceiver {
    void activate()   override { LOG_WRN("DDSRX","WaitSetReceiver activate (stub)"); }
    void deactivate() override { LOG_WRN("DDSRX","WaitSetReceiver deactivate (stub)"); }
};

} // anon

std::unique_ptr<IDdsReceiver>
create_receiver(DdsReceiveMode mode, rtpdds::DdsManager& /*mgr*/)
{
    switch (mode) {
        case DdsReceiveMode::Listener: return std::make_unique<ListenerReceiver>();
        case DdsReceiveMode::WaitSet:  return std::make_unique<WaitSetReceiver>(); // 스텁
        default:                       return std::make_unique<ListenerReceiver>();
    }
}

}} // namespace
```

## A-2. Gateway 연결

### `include/gateway.hpp` 추가 필드

```cpp
#include "async/receiver_factory.hpp"

private:
    async::DdsReceiveMode rx_mode_{async::DdsReceiveMode::Listener};
    std::unique_ptr<async::IDdsReceiver> rx_;
```

### `src/gateway.cpp` 수정

```cpp
bool GatewayApp::start_server(const std::string& bind, uint16_t port) {
    if (!ipc_) ipc_ = std::make_unique<IpcAdapter>(mgr_);
    if (!rx_)  rx_  = async::create_receiver(rx_mode_, mgr_);
    rx_->activate();
    // 이하 동일
    ...
}

bool GatewayApp::start_client(const std::string& peer, uint16_t port) {
    if (!ipc_) ipc_ = std::make_unique<IpcAdapter>(mgr_);
    if (!rx_)  rx_  = async::create_receiver(rx_mode_, mgr_);
    rx_->activate();
    ...
}

void GatewayApp::stop() {
    if (rx_) rx_->deactivate();
    ...
}
```

# B. 안정화·관측성(5단계, 5-2 포함)

## B-1. AsyncEventProcessor 확장(설정, 모니터링, 드롭 정책, 실행시간 측정)

### `include/async/async_event_processor.hpp` 대체/보강

```cpp
#pragma once
#include <mutex>
#include <deque>
#include <condition_variable>
#include <thread>
#include <atomic>
#include <functional>
#include <cstdint>
#include <chrono>
#include "sample_handler.hpp"
#include "triad_log.hpp"

namespace rtpdds { namespace async {

class AsyncEventProcessor {
public:
    struct Config {
        size_t max_queue   = 8192;  // 최대 큐 깊이
        int    monitor_sec = 10;    // 주기 통계 로그(0=비활성)
        bool   drain_stop  = true;  // stop() 시 큐 드레인
        uint32_t exec_warn_us = 2000; // 작업 1건 실행 경고 임계(μs)
    };

    explicit AsyncEventProcessor(const Config& cfg = {}) : cfg_(cfg) {}
    ~AsyncEventProcessor() { stop(); }

    void start() {
        bool expected=false; if (!running_.compare_exchange_strong(expected,true)) return;
        worker_  = std::thread([this]{ loop(); });
        if (cfg_.monitor_sec > 0)
            monitor_ = std::thread([this]{ monitor_loop(); });
        LOG_INF("ASYNC","start max_q=%zu monitor=%ds drain=%d warn_us=%u",
                cfg_.max_queue, cfg_.monitor_sec, cfg_.drain_stop, cfg_.exec_warn_us);
    }

    void stop() {
        bool expected=true; if (!running_.compare_exchange_strong(expected,false)) return;
        cv_.notify_all();
        if (worker_.joinable())  worker_.join();
        if (monitor_.joinable()) monitor_.join();
    }

    // 핸들러
    void set_handlers(const Handlers& hs) {
        std::lock_guard<std::mutex> lk(m_);
        sample_handler_ = hs.sample;
        cmd_handler_    = hs.command;
        error_handler_  = hs.error;
    }
    // 하위호환
    void set_sample_handler(SampleHandler h){ std::lock_guard<std::mutex> lk(m_); sample_handler_ = std::move(h); }
    void set_command_handler(CommandHandler h){ std::lock_guard<std::mutex> lk(m_); cmd_handler_ = std::move(h); }
    void set_error_handler(ErrorHandler h){ std::lock_guard<std::mutex> lk(m_); error_handler_ = std::move(h); }

    // 게시
    void post(const SampleEvent& ev)  { stats_enq_sample_.fetch_add(1); enqueue([this, ev]{ auto h=sample_handler_; if (h) h(ev); }); }
    void post(const CommandEvent& ev) { stats_enq_cmd_.fetch_add(1);    enqueue([this, ev]{ auto h=cmd_handler_;    if (h) h(ev); }); }
    void post(const ErrorEvent& ev)   { stats_enq_err_.fetch_add(1);    enqueue([this, ev]{ auto h=error_handler_;  if (h) h(ev.what, ev.where); }); }

    struct Stats {
        uint64_t enq_sample, enq_cmd, enq_err, exec_jobs, dropped;
        size_t   max_depth, cur_depth;
    };
    Stats get_stats() const {
        std::lock_guard<std::mutex> lk(m_);
        return { stats_enq_sample_.load(), stats_enq_cmd_.load(), stats_enq_err_.load(),
                 stats_exec_.load(), stats_drop_.load(), max_depth_, q_.size() };
    }

private:
    void enqueue(std::function<void()> fn) {
        {
            std::lock_guard<std::mutex> lk(m_);
            if (q_.size() >= cfg_.max_queue) {
                stats_drop_.fetch_add(1);
                if (error_handler_) error_handler_("queue overflow","AsyncEventProcessor::enqueue");
                LOG_WRN("ASYNC","drop queue_full depth=%zu", q_.size());
                return;
            }
            q_.push_back(std::move(fn));
            if (q_.size() > max_depth_) max_depth_ = q_.size();
        }
        cv_.notify_one();
    }

    void loop() {
        for (;;) {
            std::function<void()> job;
            {
                std::unique_lock<std::mutex> lk(m_);
                cv_.wait(lk, [this]{ return !running_.load() || !q_.empty(); });
                if (!running_.load() && q_.empty()) break;

                if (!running_.load() && !cfg_.drain_stop && !q_.empty()) {
                    stats_drop_.fetch_add(q_.size());
                    q_.clear();
                    break;
                }
                if (!q_.empty()) { job = std::move(q_.front()); q_.pop_front(); }
            }

            const auto t0 = std::chrono::steady_clock::now();
            try { if (job) job(); }
            catch (const std::exception& e) {
                if (error_handler_) error_handler_(e.what(),"AsyncEventProcessor::loop");
                LOG_ERR("ASYNC","exec exception=%s", e.what());
            }
            const auto usec = (uint64_t)std::chrono::duration_cast<std::chrono::microseconds>(
                                std::chrono::steady_clock::now()-t0).count();
            if (usec > cfg_.exec_warn_us) {
                LOG_WRN("ASYNC","slow job exec_us=%llu", (unsigned long long)usec);
            }
            stats_exec_.fetch_add(1);
        }
    }

    void monitor_loop() {
        while (running_.load()) {
            std::this_thread::sleep_for(std::chrono::seconds(cfg_.monitor_sec));
            auto st = get_stats();
            LOG_INF("ASYNC","stats enq(s/c/e)=(%llu/%llu/%llu) exec=%llu drop=%llu max_depth=%zu cur_depth=%zu",
                (unsigned long long)st.enq_sample, (unsigned long long)st.enq_cmd,
                (unsigned long long)st.enq_err, (unsigned long long)st.exec_jobs,
                (unsigned long long)st.dropped, st.max_depth, st.cur_depth);
        }
    }

    // 상태
    mutable std::mutex m_;
    std::condition_variable cv_;
    std::deque<std::function<void()>> q_;
    std::thread worker_, monitor_;
    std::atomic<bool> running_{false};

    // 핸들러
    SampleHandler  sample_handler_;
    CommandHandler cmd_handler_;
    ErrorHandler   error_handler_;

    // 통계/설정
    size_t max_depth_{0};
    std::atomic<uint64_t> stats_enq_sample_{0}, stats_enq_cmd_{0}, stats_enq_err_{0};
    std::atomic<uint64_t> stats_exec_{0}, stats_drop_{0};
    Config cfg_;
};

}} // namespace
```

## B-2. 엔큐/디큐 시점과 경과시간 로깅 지점(정확 위치)

### `src/gateway.cpp`

* **DDS 샘플 엔큐 직후**

```cpp
mgr_.set_on_sample([this](const std::string& topic,
                          const std::string& type_name,
                          const AnyData& data) {
    async::SampleEvent ev{topic, type_name, data};
    auto st = async_.get_stats();
    LOG_DBG("ASYNC","sample enq topic=%s type=%s seq=%llu depth_now=%zu",
            ev.topic.c_str(), ev.type_name.c_str(),
            (unsigned long long)ev.sequence_id, st.cur_depth);
    async_.post(ev);
});
```

* **DDS 샘플 디큐 직전/직후(실행시간은 AsyncEventProcessor가 경고 출력)**

```cpp
hs.sample = [this](const async::SampleEvent& ev) {
    LOG_DBG("ASYNC","sample exec topic=%s type=%s seq=%llu",
            ev.topic.c_str(), ev.type_name.c_str(),
            (unsigned long long)ev.sequence_id);
    if (ipc_) ipc_->emit_evt_from_sample(ev);
};
```

* **IPC 커맨드 엔큐 직후**

```cpp
ipc_->set_command_post([this](const async::CommandEvent& ev){
    auto st = async_.get_stats();
    LOG_DBG("ASYNC","cmd enq corr_id=%u size=%zu depth_now=%zu",
            ev.corr_id, ev.body.size(), st.cur_depth);
    async_.post(ev);
});
```

* **IPC 커맨드 디큐 직전**

```cpp
hs.command = [this](const async::CommandEvent& ev) {
    LOG_DBG("ASYNC","cmd exec corr_id=%u size=%zu route=%s",
            ev.corr_id, ev.body.size(), ev.route.c_str());
    if (ipc_) ipc_->process_request(ev);
};
```

* **종료 시 최종 통계**

```cpp
auto st = async_.get_stats();
LOG_INF("ASYNC","final stats enq(s/c/e)=(%llu/%llu/%llu) exec=%llu drop=%llu max_depth=%zu cur_depth=%zu",
        (unsigned long long)st.enq_sample, (unsigned long long)st.enq_cmd,
        (unsigned long long)st.enq_err, (unsigned long long)st.exec_jobs,
        (unsigned long long)st.dropped, st.max_depth, st.cur_depth);
```

### `src/ipc_adapter.cpp`

* **수신 즉시(큐 적재 전)**

```cpp
cb.on_request = [this](const dkmrtp::ipc::Header& h, const uint8_t* body, uint32_t len) {
    LOG_DBG("IPC","on_request corr_id=%u size=%u", h.corr_id, len);
    async::CommandEvent ev; ev.corr_id=h.corr_id; ev.route="ipc"; ev.body.assign(body, body+len); ev.is_cbor=true;
    if (!post_cmd_) {
        LOG_WRN("IPC","command post null corr_id=%u", h.corr_id);
        // 에러 응답 즉시 발송
        ...
        return;
    }
    post_cmd_(ev);
};
```

* **실제 처리 시작/종료(소요시간 측정)**

```cpp
void IpcAdapter::process_request(const async::CommandEvent& ev)
{
    const auto t0 = std::chrono::steady_clock::now();
    LOG_DBG("IPC","process_request corr_id=%u size=%zu route=%s",
            ev.corr_id, ev.body.size(), ev.route.c_str());

    nlohmann::json rsp;
    // ... 기존 요청 처리 분기

    auto out = nlohmann::json::to_cbor(rsp);
    ipc_.send_frame(dkmrtp::ipc::MSG_FRAME_RSP, ev.corr_id, out.data(), (uint32_t)out.size());

    const auto exec_us = (long long)std::chrono::duration_cast<std::chrono::microseconds>(
                          std::chrono::steady_clock::now() - t0).count();
    const auto q_delay_us = (long long)std::chrono::duration_cast<std::chrono::microseconds>(
                             t0 - ev.received_time).count();
    LOG_INF("IPC","process_request done corr_id=%u q_delay(us)=%lld exec(us)=%lld rsp_size=%zu",
            ev.corr_id, q_delay_us, exec_us, out.size());
}
```

* **EVT 전송 함수(참고)**

```cpp
void IpcAdapter::emit_evt_from_sample(const async::SampleEvent& ev) {
    // 이미 2단계에서 구현. 원형 유지.
    // 필요 시 display/data 크기 로그 추가:
    // LOG_DBG("IPC","evt payload size display=%zu data=%zu", display.dump().size(), out.size());
}
```

## B-3. Gateway에서 모니터링 설정값 주입

```cpp
GatewayApp::GatewayApp()
  : async_(async::AsyncEventProcessor::Config{
        /*max_queue*/   8192,
        /*monitor_sec*/ 10,
        /*drain_stop*/  true,
        /*exec_warn_us*/ 2000 })
{
    async_.start();
    // 나머지는 3단계와 동일
}
```

# C. CMake

* `src/async/receiver_factory.cpp`만 신규 포함. `GLOB_RECURSE`면 자동. 아니면 명시:

```cmake
target_sources(RtpDdsCore PRIVATE src/async/receiver_factory.cpp)
```

# D. 검증 시나리오

1. 빌드.
2. 기동 로그:

   * `ASYNC start max_q=... monitor=... drain=... warn_us=...`
   * `DDSRX ListenerReceiver activate`
3. 트래픽 주입:

   * DDS 퍼블리시: `sample enq ... depth_now=...` → `sample exec ...`
   * IPC 요청: `IPC on_request ...` → `cmd enq ... depth_now=...` → `cmd exec ...` → `IPC process_request done ... q_delay ... exec ...`
4. 주기 모니터링:

   * 10초마다 `ASYNC stats enq(s/c/e)=... exec=... drop=... max_depth=... cur_depth=...`
5. 종료:

   * `ASYNC final stats ...`
6. 스트레스:

   * 큐 초과 시 `drop queue_full` 경고, 누적 drop 증가.
   * 느린 작업 시 `slow job exec_us=...` 경고.

# E. RTI API 근거(설계 메모)

* Listener 경로: `dds::sub::DataReader<T>`, `on_data_available()`에서 `read()/take()` 사용.
* WaitSet 경로(향후): `dds::sub::cond::ReadCondition` + `dds::core::cond::WaitSet::wait/dispatch`.
* 위 심벌은 RTI Connext DDS 7.5.0 Modern C++ API에 존재. 본 단계에서는 WaitSet 스텁만 두고 동작은 Listener로 유지.

적용 후 로그 스냅샷만 주면 추가 미세 조정치(큐 상한, 경고 임계, 모니터 주기)를 계산해 준다.
